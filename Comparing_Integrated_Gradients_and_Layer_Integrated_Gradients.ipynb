{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Gradients (IG) tells us how much the inputs contribited to the final output. By using Layer Integrated Gradients (LIG) , we can find out how much each neuron of that layer for the same input. LIG can be made equal to IG by using principles of chain rule for backpropogation. In this notebook, I will attempt to show this relation. As with other notebooks, we will start with simple NN , run both LIG and IG and show how they are related <br>\n",
    "TODO : add more information on how the relation works using back propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from captum.attr import LayerIntegratedGradients,IntegratedGradients\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "inData = torch.rand((1,input_dim),requires_grad=True) # shape is [1,3] Imagine this to be a word with 3 dimensional vector [batch_size,num_dim]\n",
    "baseline = torch.rand((1,input_dim),requires_grad=True)\n",
    "target_index = 1 # index of second class\n",
    "num_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNN,self).__init__()\n",
    "        self.input_linear = nn.Linear(in_features=input_dim,out_features=3,bias=True) # takes 2 features and outputs 3 \n",
    "        self.hidden= nn.Linear(in_features=3,out_features=2,bias=True) #one output for each class\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.input_linear(x))\n",
    "        x= torch.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_i(inputs):\n",
    "    logits = net(inputs) # I cant pass the model itself as a parameter to IntegratedGradients\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simpleNN(\n",
       "  (input_linear): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (hidden): Linear(in_features=3, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNN()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(predict_i,net.input_linear)\n",
    "attributions_dim_linear_layer = lig.attribute(inputs=(inData),\n",
    "                                  baselines=(baseline),\n",
    "                                   n_steps=num_steps,\n",
    "                                   target =target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribution of each nueron in the linear layer\n",
      "\t -0.00461\n",
      "\t -0.0\n",
      "\t 0.00245\n"
     ]
    }
   ],
   "source": [
    "print(\"Attribution of each nueron in the linear layer\")\n",
    "\n",
    "for att in attributions_dim_linear_layer[0]:\n",
    "    print(\"\\t\", round(att.item(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions associated with each dimension of the word are\n",
      "\t -0.00015\n",
      "\t -0.00201\n"
     ]
    }
   ],
   "source": [
    "ig = IntegratedGradients(forward_func=predict_i)\n",
    "attributions_summary = ig.attribute(inputs=(inData),\n",
    "                                  baselines=(baseline),\n",
    "                                   n_steps=num_steps,\n",
    "                                   target =target_index)\n",
    "print(\"Attributions associated with each dimension of the word are\")\n",
    "for att in attributions_summary[0]:\n",
    "    print(\"\\t\", round(att.item(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relating IG and LIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution sum of each embedding dimension of the input from IG\n",
      "-0.00216\n"
     ]
    }
   ],
   "source": [
    "print(\"attribution sum of each embedding dimension of the input from IG\")\n",
    "print(round(torch.sum(attributions_summary[0]).item(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution sum of each neuron in linear layer from LIG\n",
      "-0.00216\n"
     ]
    }
   ],
   "source": [
    "print(\"attribution sum of each neuron in linear layer from LIG\")\n",
    "print(round(torch.sum(attributions_summary[0]).item(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert round(torch.sum(attributions_summary[0]).item(),5) == round(torch.sum(attributions_dim_linear_layer).item(),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thus proved .. Voila !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By principles of back propogation , we know that the gradients (indirectly attributions) present in linear layers are divided among each dimension of the input according to how much each dimension contributed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpret_k1",
   "language": "python",
   "name": "interpret_k1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
