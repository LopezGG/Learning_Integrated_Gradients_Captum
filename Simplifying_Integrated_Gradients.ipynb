{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this notebook, we will be simplifying some of the common techniques used in Captum for interpreting BERT<br>\n",
    "We will start with a simple Neural Network and then proceed to  Integrated Gradients. There are 3 sections:\n",
    "    1. Using Intergrated gradients from Captum Library\n",
    "    2. Writing a simplified version of Intergrated Gradients\n",
    "    3. Expanding the library function from captum without bells & whistles <br>\n",
    "The results from all the 3 methods can be found in the last cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from captum.attr import IntegratedGradients\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Simple Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNN,self).__init__()\n",
    "        self.input_linear = nn.Linear(in_features=3,out_features=3,bias=True) # takes 2 features and outputs 3 \n",
    "        self.hidden= nn.Linear(in_features=3,out_features=2,bias=True) #one output for each class\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.input_linear(x))\n",
    "        x= torch.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inData = torch.tensor([[ 0.5, 0.3,0.6]],requires_grad=True) # shape is [1,3] Imagine this to be a word with 3 dimensional vector [batch_size,num_dim]\n",
    "baseline = torch.tensor([[ 0.3, 0.7,0.2]],requires_grad=True)\n",
    "target_index = 1 # index of second class\n",
    "num_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Intergrated Gradients from Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_i(inputs):\n",
    "    logits = net(inputs) # I cant pass the model itself as a parameter to IntegratedGradients\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing this to Integrated Gradients method\n",
    "ig = IntegratedGradients(forward_func=predict_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions associated with each dimension of the word are\n",
      "\t 0.0015\n",
      "\t 0.00244\n",
      "\t -0.00217\n"
     ]
    }
   ],
   "source": [
    "attributions_summary = ig.attribute(inputs=(inData),\n",
    "                                  baselines=(baseline),\n",
    "                                   n_steps=num_steps,\n",
    "                                   target =target_index)\n",
    "print(\"Attributions associated with each dimension of the word are\")\n",
    "for att in attributions_summary[0]:\n",
    "    print(\"\\t\", round(att.item(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Intergrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will recreate the results in understandable terms.  We start with creating 10 vectors which lie between input and baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = torch.linspace(0, 1, num_steps).unsqueeze(-1) #[10, 1] i.e 10 columns , equally spaced points between 0 & 1\n",
    "#10 points between zero & 1. we stack it vertically thrice because each datapoint has 3 dimensions\n",
    "points = torch.cat((dim1,dim1,dim1),1).float() # [10, 3] [batch_size,dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the data points shown below , first row is the baseline and last row is the input Data\n",
      "[[0.3        0.7        0.2       ]\n",
      " [0.32222223 0.65555555 0.24444446]\n",
      " [0.34444445 0.6111111  0.2888889 ]\n",
      " [0.36666667 0.56666666 0.33333337]\n",
      " [0.3888889  0.5222222  0.3777778 ]\n",
      " [0.41111112 0.47777778 0.42222226]\n",
      " [0.43333334 0.43333334 0.4666667 ]\n",
      " [0.45555556 0.3888889  0.51111114]\n",
      " [0.47777778 0.34444445 0.5555556 ]\n",
      " [0.5        0.3        0.6       ]]\n"
     ]
    }
   ],
   "source": [
    "delta_points_np = (baseline + points * (inData - baseline)).detach().numpy() #(10, 3)\n",
    "print(\"In the data points shown below , first row is the baseline and last row is the input Data\")\n",
    "print(delta_points_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Intergrated Gradients formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/4RD4RXhpZgAATU0AKgAAAAgABAE7AAIAAAAPAAAISodpAAQAAAABAAAIWpydAAEAAAAeAAAQ0uocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEdpbHNpbmlhIExvcGV6AAAABZADAAIAAAAUAAAQqJAEAAIAAAAUAAAQvJKRAAIAAAADNDkAAJKSAAIAAAADNDkAAOocAAcAAAgMAAAInAAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIwMjA6MDk6MDcgMTY6MjE6MjcAMjAyMDowOTowNyAxNjoyMToyNwAAAEcAaQBsAHMAaQBuAGkAYQAgAEwAbwBwAGUAegAAAP/hCyFodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDIwLTA5LTA3VDE2OjIxOjI3LjQ4ODwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5HaWxzaW5pYSBMb3BlejwvcmRmOmxpPjwvcmRmOlNlcT4NCgkJCTwvZGM6Y3JlYXRvcj48L3JkZjpEZXNjcmlwdGlvbj48L3JkZjpSREY+PC94OnhtcG1ldGE+DQogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9J3cnPz7/2wBDAAcFBQYFBAcGBQYIBwcIChELCgkJChUPEAwRGBUaGRgVGBcbHichGx0lHRcYIi4iJSgpKywrGiAvMy8qMicqKyr/2wBDAQcICAoJChQLCxQqHBgcKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKir/wAARCACVAvkDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD6RooooAKKKKACiiigDz//AIRb+0riyt4vDd3pMMF5HdySXV8ssEOx9+23iWVghY8ZCphS3+6e2i02zhkvHjgGb599xuJYSHYE5B4+6oGBVqijpb+v60DrcxLDwhounXVtc21tK01oCLd57qWYwqV2lV3sdq4/hHHTjgVHP4I0C6MonspHilk802/2qXyVfdu3rFu2K2edygHk+pzv0UCsilJo9hM9601qkv29FjuVkyyyqAQAVPGME1WsvDWm2EcyRC6lWaIwt9pvpp8IeqqZHbaD/s46D0Fa1FKyGUX0awkgsIXgzHp7rJaje37tlUqp688EjnNQS+G9NlnvJSlyjXpBnEV5NGpYbcMFVgFb5V+ZQDx1rVophtoUdM0ey0hZRZJJumYNJLPO80jkDAy7ksQB0BPHaob/AMN6XqV99suoZROUCO0NzJEJlByFkCMBIvJ4YEcn1NalFAbGQfCujf2NbaWloYrW0bfbCGZ43gbnlJFYMvUjg9CR04qVPD+nLposWillgEyznzriSR2kVgysXZizYKr1J4AHTitKigDPutD068e+a5t97ahbrbXJ3sPMjG7C8Hj77cjB5+lSz6ZZ3NxZTzQ7pLFy9u24jYShQnrz8rEc5q3RTuwMiLwtpUGom8ginjcymYwpdyiDzDyW8nd5ec8529eevNLbeGdLtdRW+SO4kmR2eMT3k0yRM2clEdiqHkjKgYBI6GtailsBW1HTrXVbF7S+jMkLkEhXZGBBBBDKQVIIBBBBGKZaaTaWVybmBJDO0CW7SyTPIzIhYqCWJJOWbk8nPJNXKKAMiTwtpEmmx2P2Vkhina4iMc8iSRyMzMzLIrB1JLN0I4JHTirun6db6Xa+RaCXaWLM00zzOxPcu5LHsOT0AHarVFAGfLoOmzfafNtt32q4jupv3jfNJHt2N14xsXgccc96sixtxqJvxH/pJiEJfcfuA5xjp1NT0Uf1+gHLa14UgupPD9na2e7T7O+kmmAlIMQMUuHDE7s+Y6kFTkHBGMcbFloGmWFrcQQ23mJdcXDXMjTvOMYw7yFmYY4wScDitGijpYDK07w1pml3K3FrFM0iKUjNxdSz+Sp6rGJGIQHA4XHQegqzDpNjBpLaZHbr9jdXRoWJYMHJLA59cn86uUUb7gtHdGLJ4T0qS1toWF7/AKJuEMw1C4EyBuq+aH3lTgfKTjgccCpbPwzo+nx2kdlYrAlncSXNuiMwWORwwYgZ7h246DPA6Vq0UXDpYovo2nyzXss1rHKb+NYroSZZZVUEAFTxjBPbmq9t4Y0u2tbm28u4uYLmPypY7y7luVKc/KBIzbRz0GO3oK1qKAM7TNCstIkd7Q3Tu6hN11ezXBVR2XzGbaPpjOB6CptS0qy1eCOHUYBNHHKsyAsRh1OQeD+nQjg8VbooAzdT0DT9WuIbi8jmE8KsiS29zJA21iCVJjZSVJA4ORxTbHw3pOmR2kdhZLBHZPK9uiMwWMyEl8DOMHceOg7YrUoo6WDcw5/B2h3NxNLPayOJ5RM8RupfK8zIbeIt2xWyAdwAPX1OZ9R8NaZqd4bu4S4juGQRvJa3k1uZFHQN5bLuAycZzjJx1rVooAaiLHGscahUUBVUDAAHas9/D+mSaPHpjW3+iwkNEqyMGjYHIZXB3KwJ4IORWlRQ9dw2MuLw3pcWm3dj5EkkN6hS5aa4klklUjGGkZi54OBzx2xWmqhECqMBRgClooAzZfD+nTX11dvFL5l5EYbhVuJFSVdu3JQNtLYAG7G7AxmppdJsphYiWHd/Z7iS2+dv3bBSgPXn5WI5z1q5RQBkDwtpS6k18kU8cjzee8cd3KsLycfOYQ3lk5AOSvJ5681oX1lbalYT2V9Es1vcIY5Y26Mp4IqeijpYOtzPl0LT59JbTZYXa2YhjmZ9+4EMG8zO7cCAd2c8daktNJtLK5NzAkhnaBLdpZJnkZkQsVBLEknLNyeTnkmrlFAeRW0/TrXSrT7NYReVD5jybdxb5nYuxySTyzE1F/Y2n+Xfxtao8eouXu0ky6ykoqHIPGNqgYHHFXqKAMmz8M6XZRTRpHcTpNEYXW8vJrkeWeqjzWbAPcDGcDPQUzT/AAppGmXkN3bQTNcwI0cU1xdSzuiEAFQZGJC8DjoK2aKAMkeF9J+w3lmYJWtr1zJLC1zKVVi27KAt+7O45+Tbg89qLTwxpNm88kUEjy3EH2eaae5klkkjyTtZ3Ysepxzx0Fa1FKyYFG40XT7vSE0y4t91pGqLGm9gU242lWB3BhgYYHIIzml03SbXSY5FtPtDGRtzvc3Mk7njAG6RmbHtnHJ9TV2iquxWWxUu9Ksr+8tLq7gEk1k5kgfcRsYjB6Hn6HPIB6gVU1DwxpOqXrXV7bu8kkYilCXEiJOgzhZEVgsg+Y8MD1Na1FIZQsdD0/TmtjZ2/lm1tRZw/Ox2xDGF5PPQcnn3qlbeDtDtZ4ZYbWTNvL51uj3Urpbtz/q0ZiqD5jwoA6ccCtyijrf+u4GRP4X0q41F71op0lkdXlWG7mijmYYwXjVgjnAAO4HIAByK0L6xttSsJrK+iEtvOhSRCSMg+45H1HIqeijpYOtzEi8IaNC00iwTtNNA1vJcSXkzzNGSDt8xnLcEcc8dsZq3pmh2OkPLJZpM0s2BJNcXEk8jAdF3yMzbRk4GcDJ9TWhRQFis+n2z6nFqDRZuoYnhSTceEYqWGM45KL+VZ48J6ML77ULaQHzfP8j7TL9n8zOd/kbvL3bvmztzu+brzWzRQBn3Oh6dd/bftFvv+3BFuPnYbwn3eh4x7YpmpeH9O1W5S5uUnjuFTyxPa3UtvIUznaWjZSVzzgkjNadFKyAzbnw9pV3pUGmz2MZsrd45IoVyqoyMGUjGOhH49+tSjRtPGutrItl/tBrcWxnyc+Xu3bcdOvfrV2imBj2vhbSrOcyxxXEj7GjT7ReTTCJWGCEDuQmRx8uOOK0NPsLbS9Ot7Cxj8q2to1iiTcW2qBgDJyT+NWKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACis7WNTk0yOJ0gidHYh5Z7gQxxYUkbmIPUjAwO9Z8Xio39vA+i2DXs0tgt/5LTCMhG4VQTkFiQRzgcHmjcDoaKxo9buru8mj0/TvPhguPs0srThCjhNxOMHKglVyDnJOAQKrWHiW8vW04HToYzepOP+PonZJExG3/V8qcfe4I9KV9LivrY6KiuasPE9/fLYD+yoY3vopXQG8JCGMgMGOzpzwQCemQOxb+Kri9NibXToyl9pb38JkuSpDKUzGwCHA/eD5hnp0p3ur/1/WhSTex0tFchbXqz69ba3baeDcXWgm4MMZXfIdyMqbjjPXAJp1/4ruJtE1NtMitWvLNolfbdb0Ak4yGVfvDkYxxjND0Wv9atfoQpJ/wBeSf6nW0Vz2oeKJbC6Fj/Z7XN+lsLmaC28yQKpJACssZyTtbG4KOOtJ4xihuNGsZZIVZo9TsnjMifNGTcRjIz0OCR+NNK7S7u342L62OiorBTxTAY4Lp41XT7i5e2in8zLF1LDJXHCkowByT0454WDX7240s6hFo7PbSWy3NuwuFBcMeFYHG07SGPUDnnNIRu0Vxesapaa/oqyCK3kl0/W7OLfG4lQP50R3I+B/C4HQdxXSapqbWEtnBDAJp7yUxxh3KICEZySwBxwp7U7Wjd/1on+ozQorlrfxXqV3DpklposbjUrZ7iJWvNrAKFOCCmOQ3r1HpzVw+Jlh16LTbu3SBp5nhi/0hWkbahfeUHRCFbBznIHHNLrYRu0VyP28axrHhzVI7KFUnaZrWYyZkaMwuQG+X5QeDgE1Hp/iiGy0qystP0lln8iWT7JC0kqxqkhTAZYyTlgcZAHuKNtxX1a/rp/mdlRXNyeKbmGW983TFSK0EDktOQ5SU4Hy7OGHOVz6c+ixa1qUWp6811aRPZ6eAYxBMzyMPLDgBNnJOex46c9aTaTsxrXY6OisvQdbi16ykuraS1liWTYr2tx5qngE54BUgnGCM8VqU9gCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiuOsfD2jXXjfxEbnSrKU7bZsvbqSCVYk9OpPfrSA7GiuG0XU5ZU0Vbu1S9kj1O9sbe6knZXj8sShWxg78pGVJJ6881nQwTXcGlSXemWjtqt/NBeZvpMTKizkbv3fY7iP8AdTnptqwbOz/qx6VRXJW3jBpdHku9M0qWS0js3uLeSQyxo6pjAZzHgEryMFunNWJfFc9u1tBPphW8u0eWGFHeUNEoTLEpGxHLgY2+vPTK06Br1Olorlp/F91bi+kl0jy4rPTo9QYSTlZNjb9yFNnDgoeMkH1FaJ1yabUJ7fT7E3UdpNHDcv5wQoXCsSARhgqsCeQewzTswszYorB0PxONcmia3sZ1s54mlhuTHIFIBGMkoF5ByMM3Q1i69fXFr4j1a+l0y3vE0bT4byDfePGynM2cAIQCdpB7HC/grXGk2dxRWKmt3bXkdg1hEl/JHJMImufk8pSAG3BScksOMdjzxzR/4S+WfElhp8c0D6e18jvc7DhDh0YBDggnAwSDz07ptLf+v6sJanUUVk6ZrT398beW1EG61juoyJdxKOSAGGBhhjsSPesmS4k0rxb4gu7SxilRbG2nnw+xnx5uSPlO5sDuR0AzT21Yr32Osorn28VxJqFtE8CLb3ckUdvI1wvmSiRQVdY+u3J2k5B68YqqfHdki3zusX+htskgSYmdXMgjRWi2gjcTwRkfWge51VFcjf3f9qvpc91pzW81rq0aRPLEwyCpyULorY5weB070abrktnaWENno0ES3Wp3VnIi3jERyK0rFwSnzBijHtjPQ0LW/wDXRf5hb+vv/wAjrqK5k+M0MdtHFZs97cNcr5C73A8iTy3OURjjdjHy9+1b9lcG7sYLhoZIDLGrmKVcOhIztI9RQD0dieiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoopGZUQu7BVUZJJwAKAIZbXzby3n8+ZPI3fu0fCSZGPmHfHUUR2vl309z9oncTKi+SzZjj255UY4Jzz9BXKXvxa8DafcGC48QQl1JB8mKSUfmikVX/wCF0+AP+g//AOSc/wD8RQB0p0CJri3uHvbx57Z5mjkd1YgSnleVxgcAdwBjPXNey8KWunR24sru8ia3h+ziQOpZos5CHK4wOxGCOeeTnC/4XT4A/wCg/wD+Sc//AMRR/wALp8Af9B//AMk5/wD4igDqYNGhtb+a4tp7iJLiQSywK42O4AG7puGQBkA4OOnJzVXwvDFcWkttf3kH2R5WiRDGV/eElgcocjJ+o9etYH/C6fAH/Qf/APJOf/4ij/hdPgD/AKD/AP5Jz/8AxFAG5a+FY7NrExapfn7FHJHHuEPIkOST+768DHTpTrXwpa2dnZQRXd3usrV7SKYsm8xNtyDhcfwLyADxWD/wunwB/wBB/wD8k5//AIij/hdPgD/oP/8AknP/APEUraWHdo1k8FWn2NLWfUb+4hSwOn7ZDEMxHHUqg54H5cg81al8L2073jS3V0xvIEhk+ZQBsOVYALgMPy9q5/8A4XT4A/6D/wD5Jz//ABFH/C6fAH/Qf/8AJOf/AOIqm77k2SOmOgw/aEuIrm4iuBG0UkyFQ0ylixDfLj7xJBABGTjFSavpEer2cVs9zPbpFNHMDBtyTGwdQdynjKiuTPxh8MXTiHQf7Q1u6bhILKyk3E++8KB9avrfeLrezOtXOnrMHPzaHE6GSGPsyyY+eTqSpOMEAYIJZbDNdPDdkkkeWle3ime4jtWK+Wkjbst0yfvMcEkDPTgYjPhe1bSBpj3V21omwQxl1/chGDKB8vzAFVHzbuBjuc88PjJ4Oiymp3d3pk44a3vLGUOv1CqR+tL/AMLp8Af9B/8A8k5//iKANu48JW9w1y39oX0ZubqG7k2NHgyR7dpwUOP9Wmfp2ycu8UaVPq1vawx+d5KSFpDbrC0mcYGBMChHJzkZ6YrC/wCF0+AP+g//AOSc/wD8RR/wunwB/wBB/wD8k5//AIijpYdza0/RLySWwvdS1C7FxZCRI49sA3xsR/rNqY3YAB2EDjipP+EVsxLG6XF0nlXr3qKrrxI4YNztyQQ7Dk8Z4xgYwf8AhdPgD/oP/wDknP8A/EUf8Lp8Af8AQf8A/JOf/wCIoEb6+F4Yn0/7Lf3lvHpzMbeKPyyqgqV28oTjBwOc+9Fp4Xt7JIPs97drPCJFFxmPeySPvZT8m3G7kcZHr1rA/wCF0+AP+g//AOSc/wD8RR/wunwB/wBB/wD8k5//AIigDdu/CkN216TqN9GLxYldUMZCiM5UDKE/XJNWJNAgkurmc3Nyr3KAPtZR84TZ5g+XIbbx6e1c1/wunwB/0H//ACTn/wDiKP8AhdPgD/oP/wDknP8A/EUAdfZafHZPPIJHlluGDSySBQWIAA+6AOg9Kt1zOi/Ebwl4hukttJ1y3lnkO1InDRM59AHAJrpqACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsMeGnTVLy/h1zUopLzb5ioINoC5CgZiJGAcdc+9blFAGO/hu1C6YlrNcWsemSGSFIipDMUZSWLKSeHbnPJOahg8KRQR6ei6lfMNPuZLmLd5XzM4YEN8nT943THWt6indgYx8NQDS59OhvLuG1ljeJI0KYgVuoTKn3A3ZwDxipToUZjsj9ruRdWSFIrseX5hU4ypG3aQcD+HsO9alcZrnju7TVJtJ8G6HN4gv7dtly6v5VvbNgHa0h4Lc8qDx3OeKQGpf8AhKDUGvvM1C+Rb6yWykVDGcRjd0LITk725JPWrkOiRwXr3MV1co02w3CqVCzsoADN8vBwADtxkAZriv7b+Lfbwro3/gV/9spP7b+Lf/Qq6L/4Ff8A2ygNzutO0eHTAI7aab7Om7yrdiNkQJzhcDOPTJOOgqjqfhWHVJtTeXUL2IalaraTLF5WFjXd93KEg/O3Jz1rk/7b+Lf/AEKui/8AgV/9sq3baR478SyRr4tvbLR9Mzmay0ot5twP7jSEnap6Hac/zoGm0dPFYWuoxWl9Z6nJLLHE0KX0DRsZEJG4HClDyo6Dgj61A3hK1GFtru7tolsmsVijMZCxtgk5ZCS2R1JNZOqeFNa0q+k1LwDf29m0mPO0q7Qm0lIGNyheY2wBnbwcVlnWvi2rEf8ACL6I2DjcLrg+/L0mr7/1/VxbHZ2GgpYX6XS311MyWqWuyXy9pRSSCdqA55Pei80C3vbq6ma4uIjeQrBcLGygSRru+XkEj77cgg89elcX/bfxb/6FXRf/AAK/+2Uf238W/wDoVdF/8Cv/ALZTBabHWS+E7JxMIp7m3Wa4juSsLKNsiBVUglScYQcflipJfDFlcqVvZZ7rETRRmQruiUsrfKQAc7kUgknG0Vzdp491rSLiGH4geHW0mGZgiajayie3Dk4AfGSmfUk/1rvaAMq50L7Wtt52pXhe3uFuBJ+7y7KMAEbMY+gFVovCcMQttupXx+zX0t8ufK+aSTduB+Tp+8bgY69a3qKFpsH9fn/mzDtPCtvZxxeTfXnnwzTSx3J8vevmsWkX7m0qWOcEdh6VtRRiKJY1LEKAAWOSfqadRQAUUUUAFFFFABRRRQAUUUUAFFFFABRnnHekd1jRnc4VRkk9hXEeE9M/4SKGz8WXdxPHczXc11EEIAaA7o4ozkZ2hMNgY5JPejqHQ6HTby9l8QapZ3ksUkdukLReXEUwH39ck5PA5/QVr1j2NlqMPibUby4itRa3SRKhSdmcbN3VSgHO71rYoEr9QooooGFFFFABRRRQAVjeIPDq+I/stte3TrpiMXubNBgXZ42KzZzsBySv8XHpg7NFAGdu0bw3ZxxFrHSrYkJGpKQqT6DoM1oKyugZGDKwyCDkEVxHizXxa3S6la2/27T9Pl+w6qrKDGySlQQM/eZDtJ4xgkZz06TXL19D8M3V5YwQFbO3aQRu5jVUVSeMKegHAx+VJu0eYaTcuVF2G+tLm4mt7e6hlmgOJY0kDNGf9oDkfjTorqGa4mhicO8BAkAHCkjOCemcYOOuCPUVyXg7TjoPg1NS1Cyt475bAPNcJKXeYAGQ78qMHczHHPXrWr4aL2fgqyuZ0ea4mtxczCMbmklkG9sfUtVNWbXb+v0JTTs+5u1Cl5bSXktpHcRPcwqGkhVwXQHoSvUA1zdz4u1C21C7sRoTXVxaeS8q2s5kGyViq4+QHcMEkEAADO6tqPS4l8Qy6oIIEleAQ+YiDzJBnPznHIGBgc9/WkM0KKrXepWNgVF9e29sXztE0qpu+mTWRp3iTSjc6j52s2ZUXWI910mAvlp056Zz+OaNwOgopiyxvCJkkVomXcHDZUjrnPpXNS+MiNEk163sRLocZJa5M22Row20yJHtIK9+WBIBOOmQDqOlRWt1DeW6z2z+ZE+drgHDDPUeo9D0Paue8T6n9p8NmOyLAXt7FYCQHGVeUI5U/Td+VbdzLJaQxQ2FqssjfLGhYpGgA/iYA7Rxjoe3FHS/9f1qBbormtE8WXOs3Vqq6NNHazmdGulcuiPE2087QNpOdpyCcfdFdHJLHCu6V1Rc4yxwKAHUVB9utP8An6h/7+CplZXUMhDKRkEHINAC1DPeW1rJCl1cwwvO/lwrI4UyNjO1c9TgdBWJqHiW6sdQs7ddL8xb+V4bbMxVyyqWLOm35UwpOQSeR8vNPWD/AISjR9Lu7/S1tZVmS5MV2mZLcqcgrkZDHAGeDgnvxQgNmK5immlijfMkJAkUggrkZHXt79KIbmKd5UifLQvskXBBU9eh9jn3rE1eT7D4v0K4jyPtrS2MuOjDy2lUn6GMgf75pb9zZeONKkj4XUIZraYAfeKDzEP4YkH/AAKgHob9FFFAGNrnhHQvEds0Or6ZbzkjCy7AsiH1VxyDV3SLO50/SYLS9vWv5oVKG5dNrSAE7S3Jy2MAnuQTgZxVyigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKhF0hvmtAGMiRiRj2AJIH54P5Vm6/rMunSafZWSI99qVx5EHmfcQBS7uw6kBVPAxk4GR1rCku73R75dQ/tGa+F7qsen+RLHGNyZ2ErtUEFW3tzkYBzzzQtXb+v61B6L+v66MspaWNz8RdT1OW0gdtMsYoxL5QLiRtzsc46hQn51XsJtNuvCdv4l8VxCVr5RcKzwNMLRH5RUAB2YBHzDGTznpWzomjjTdS1Zzc39y13Mskr3axbHbYq/JsUHACgYPp9aF8K2gtFsWubp9ORlMdiWURx7WDKAQocgEDgsR26cUraW/rzB73/ryLVl9n0Xw3CZ538i1tg0k84IYgLkswPOTyTnnNWrKeS6s455oGt2kG4RMcsoPTPocdR2Pc1ieNSzaRZ2wJCXWpWsMnGcoZVJB9iBj8a6CRS8bKrtGWBAdcZX3Gcj86d73fn/AMH9Q8jIk8UWSeJE0WOK5mnLbJJIot0cLFC4Vzng7RnpjketbNcZ8PbOd4NT1eXULmf7dqNw22RYwJFRvKVzhAc7Yx0IHtXZ0dEHVhRXLeIb3VrLUNMS1vvKl1C/FqluI1dBFtZmckru3BVJ64BwMHnOzosOpwab5et3MdzciR8SRjGU3HZnAAztxnAAzQtQejNCioboXDQFbRlSRiBvb+EZ5IHc46ds1yWlXHiLUtSuxZ6qkllp+q/Zi1xEgaeJUBlB2rgsHO1cbcYOc0LV2/rp/mD0VzoIdZ+06lqOnwWV0txYqrBriMxxXG4ZGx+cjPBOODVrT7+HUrFLq2J2NkFWGGRgcMpHYggg+4p8dqkd3Nc7maSUKp3HhVHRR7ZJP4/TGH4aYprnia1BzHFqKugB+7vgjZh/30SfqTQu3kPzR0VFZ2rXv2MRf8TSw0/dn/j8XO/p0+den49azf7c/wCpo0D/AL9f/b6BHR0VFayedaRSedFPuUHzYRhH9xyePxNS0AFFFFABWPBFp/hq2stI0i0C+c5EUCsenV5GY5J68sckkgdTWxXOQOZ/iZeq/S00uER5HTzJZC3/AKLX8qOodGzo6K5fxXfatpyW01jeLFNcX0NpbW/lq6SB2G4vkbshd5+Uj7o681dtjqunaVdLqup2b3LzutlNOAF2n/Vq+AuW9QuPT3oWv9en+YPT+v67G3RR9aotqe1iPsN4cHqIf/r0AXqKr2139pLD7PPFt7yptz9Kqa288NhNOt6bG2t4XmlnjClxtGcDepXGM549KTdldjSu7I0ZH8uNn2s20E7VGSfYVT0XVo9b0mG/ht7m1WXcDDdxeXLGQSCGXscisfwq/iO9gs9R1m7gNvd6fHI9usYVo5m5444G3Gck856CtaRV0zT4re3uYY5ncKj3bf612bLdCMsck8dzVNW0ZN7rQS/ntpbldK1O3R7e/jaNPM+ZZTgloyMf3ckeoDenNy1torKzhtbcMIoI1jQM5YhQMDJJJPA6k5rC8bMYdDt7qM7ZLfUbR0OfWdFI/FWI+hroqS2GFFFFABRRRQAUVh6rqsugXBmuCbq1uDhIwQHhcD9Y+Mkn7vJPy/d07GO4S33Xk6yzSHe2wfIn+yvsPU8nk+wALNFFFABRRRQAUUUUAFFFFAHL/EjVTo3w51m6RtsjW5hjI67pPkH/AKFWxoGnjSvDenaeowLW1ji/75UCuP8AimTfS+GNBTn+0tXiMi+sUfzN/MV6BRHZvz/Jf8EHul5fn/wwUUUUAFFFFABRRRQAUUUUAFFFMnkaG3kkjhed0QssUZUNIQPujcQMnpyQPUigDlY/Cd0NPl0WRoDpct813LN5jedIGm83yyu3HX5d27oOnNN8d6hb3mlXPhm2l36jfCGIwKDkRSyBGbPTG0OevY1pf8JBqX/Qoa1/3+sv/kioG1O4a9W8bwPqhulXYs5ax3hfQN9ozj2o7B3fU27yzS60uey4VJYWi+gK4rB0sarefDq3ttLuI7PV4bdbYySJuWKWPCOCMH+6ccHqDg1a/wCEg1L/AKFDWv8Av9Zf/JFQxateQ3E00XgzWUecgyETWeGIGMkfaMZxgZ64A9BR38/6/UO3kR6Vous2WrajdTS2gOpTxzSzRuxeJURV8pVK4YYU/MSPvE7a6esL/hINS/6FDWv+/wBZf/JFH/CQal/0KGtf9/rL/wCSKAN2qWn20sFxqDSrgTXPmJz1Xy0H81NZ/wDwkGpf9ChrX/f6y/8Akij/AISDUv8AoUNa/wC/1l/8kUAa97bLe2FxauzKs8TRll6gEYyPzrlH8L6tP4Abwo72MEK2Is1u0d3aQBdoJTau0nAJ+Zsc9a1P+Eg1L/oUNa/7/WX/AMkUf8JBqX/Qoa1/3+sv/kilbfz/AK/Udyhren3cPgm1LQQRzaXPb3Pk2xLqEikUsFJAJOwN2H9a1NWi1i7ksZdBvbaO3+c3CyDPmqy4VgcHoTuwMZ4GRUX/AAkGpH/mUNZ/7/WX/wAkVFa6teWdusFt4N1mOJM7UE1nhRnoP9I4HoOg7U3re4lpsP8AC2j6homk2WnXMkAgsofKBhYsbhv+ejZUbT1O0Z5brxW7JFHMu2VFdc5wwyKxP+Eg1L/oUNa/7/WX/wAkUf8ACQal/wBChrX/AH+sv/kim3cDW+w2n/PrD/37FTKoVQqAKAMAAcCsP/hINS/6FDWv+/1l/wDJFH/CQal/0KGtf9/rL/5IpAZkGheJv7Y0/U9TubC/nsUuIkUs0YYSMCJOE4IUBduDxk7q6y3SSK2jSeXzpVUB5NoXce5wOlY3/CQal/0KGtf9/rL/AOSKP+Eg1L/oUNa/7/WX/wAkUdLB1uN1eI33i7Q7dMn7E0t7Ljoo8tolB+pkJH+4aL9De+ONKjj5XT4ZbmYg/dLjy0B+o8w/8BpkWr3sM0ssfg3WRJMQZGM9mS2Bgdbjp7dKIdXvYHleLwbrIaZ98jedZksenU3HoMe1APU6KspdeiPio6CbS5WcWxuhMdhjKbgvZtwOTjkDoaq63rNxFr2l6Dp52XOoCSWSfAPkQxgbmAPBYllAzkc5wcYrl4NWk0O58W+I7qR9TktZYdLs96qjSsuMKdoA/wBZNgkAfd6ULf8Ar0/OwP8Ar+vQ9Horj9Z1XVPC/wDZF3f3r34vbtbW4tlhRVVnRiDFgbuGUDDM2QfWustjMbWI3QQTlB5gT7obHOPbNAElFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAGfrGmwX0Ec0nmpPZsZreWDHmRvtI+UEEHIJGCCDnpVPRvDy2tvZ3GozS3l9ChKvPtxC78uVVQBkknnGcHGcVuUUBuYPhS5e5t79nuZpx9skC+a24xhTsx7ElC+3tvHSt6iigDJ8T6ZJq2hSQ2wBuYZI7m3BOAZI3DqM+5XH41W1XxDcw6VaXGk6Xc35u2MZEYIa3Oxj84wcYYBTnGCeSMVv0Umrqw7mX4Z0k6F4X07THYPJbQKkjDoz4yx/Ekmm6NZTWl/qsj3N5NBPc74lupN2w4+YJxwmeAPYnoa1qKpu7uTbSxzaeCLGG++2W17fQzrdS3UTh0bynk4kwGUggjj5s4xxirGs6Qz6DHZ6dcXttKk6SRyWspVmfdklz3UkktnrzW5RS2sMztb0W312xS2upJohHMk6PCQCro25TyCDyOhBFV9N8MWmlviCe5eATNcLbyupRZWJLPnG4kkk8kgE8AYGNmihaBuMmmjt4XmmcJHGpZmY4AA71k+HLKSCG9vrlGjn1K6a5aNuqLgIin0OxFyPUmtmigAooooAKKKKACiiigArCvYP7P8W2+r4/cXNv9iuGA+4Q26Nj7ZLrn1Za3aKOtw6WOck8E2D6lLfJdXkU8l39sQq6kRTbdhYBlOcrxhsgdgKfrOjSHTba3sYGuibpZLhnkAd1zvYknA5ZVBx0UnA4xXQUUbAAzgZ698UUUUAFYeteE9P169W5vnuAfsz2jxxybVkicgsp4yM4HKkH8K3KKVrjvYo6dpSaf8zXE91KEEYln25VB0UBVAA/DJ75wKp+ILG5vJ9NNtbLOkdyHmJYDaFG5epHHmKhOMn5elbVFVfW4uljD12A6teWGmIMolxHd3JB+4kbblH1Z1Xj0DelblFFLpYAooooAKKKKAK0VjFHeS3R3STSjbuc52r/AHV9B39z1pbOyjsYmitywi3ZSMnIjH91fQe3bOBgYFWKKACiiigAooooAKKKKACiiigDgtRj/tX46aRCQTHo+ly3R9A8jbB+grva47wnF9s8aeLNbblGuY7CE9tsKfNj/gbH8q6K31rT7qYRw3GSy7kLIyrIuQMoxGGGSBkE9R6ihfCl/WuoP4m/600/Mv0VnN4g0ZGKvq9grA4INygIP51m32svd6tJaaXfBEh043vmwhHEhLFVGSCNvytnGD7ik2l/XzHa/wDXyOjoqnpGoLquiWWoINq3dukwAOcblB/rVyqaadmSndXQUUUUhhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFHW9Q/snRbq/ONtum9mKlgi92IHJAGTgdcUmkPqMlu76nLZThmzBNZhlWSMgEEqxOD16EjpzWXqcst/4wg0ecmPTUsXupxu2/aG3hAhP90DJI75XPHBx/CgstN8D6ZY6vculrdX0sVjE4LedEZHMSHg/KUAPpgc8cULv/W9gfb+trnd0V574R03TL6O01l5HgZ9TuptPigOFWEF0CYA+VMAtgYG5hnriuw0i8hv2vZ4PtKnz9jpO3CkKv3RkgDBBx65zzQBpUUUUAFFFFABRRRQAUUUUAZuoaLFf6hbXyTz2l5bK8aTwbdxRsFkIZWGCVU9M8cGs6LwNpMdldWjNczQXDtIqyybjCzOJCytjdkuqtli3IHbiujooAwr3wtHqUUIv9Tv55raZZ7ectGrQupyCFVAh7j5lPBPrW3GnlRIm5m2qBuY5J9yfWnUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVj61ZNqtxawWOrGyvLGVbpokcnepDKBIgYEoSD3wStbFc94o8Mzay1vf6RqMml6xZBvs90ihlYHGY5FP3kJA47daALFt4djsvCc2i2s8imaGVWuSPnaSTJaQ47lmJqsNEvZNS0a5lhs44tNjeLykmY5DBQGB2jONg+Uj0OeK5t9U+LVrIIjoGhXwTgzQ3BQSfQMwI/KtfQ/HM8+qw6P4r0W40HVJ8iAOwlguCOcJKvGcc4P60dbgbzabdMxI1u/UE9AkGB/5Cqne6Ld/bGu7OVJ55bJrOV7ltpxuJV/kXBIyeMDPqKw9S8eapeahPY+BfDsuttbuY5r2SUQ2yuOCoZvvkHg4qkl38W9SzAdO0HRw3W6eQylR/sqGbn6jFKyf9fIdzudMjtLG1h0m2nR3sbeNDHvBdUxtUkdRnaceuD6VdrG8NeHI/DtjKjXMl9fXUnnXl9MB5lxJjGTjoAAAFHAA+pOzVNtu7JSSVkFFFFIYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBWvNOstRRF1Czt7pUO5RPErhT6jI4p11Y2l9CIb21huYlIYJNGHUEdDg1PRQBXg0+ytbiWe2tIIZpjmWSOIKz/UgZP41W0bS30q2kikuftBeV5N2zb95ixzycnLHJ47cCtGigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAMfVfEH9l6pZWH9mXt3Je7hC0Bi2kqCxB3yKRwM9MVc07UDfxzeZZ3FnJDJ5bxXGzd0BByjMCCCO9cz4vu7VfF/hq3fVUsJvNnO9ZIw6gxEA4cEcnjkVJr2zTrHw7bahq0lrKblIZZUvGhMw8pg2cEbuQOo4JHSlf80vvA66iuKvru/sl1WH7TKb+0aKPSYnnO66XYnLL0cl94JIOMdsVn3OpQw3OqEa5NGYdet4EB1BiEjdYvMXBbp80n+7zjGOKSu7L+tUv1B6Jvseinp61R0jVI9Y0/wC1xQzQDzZImjm27lZHZGztJHVT0NUPDF6lydVgju/tK2t+8abpjIyLtU4JJJ6luv07VgaVI9rDbiwnlN8+tXPm2xlYgwG4kLMY84A28hsdcc80aWXnb8Q+y32f5X/yO8orzuDXblZE8vUZoYL7TbmRJrqUyyLKrKULRgARsAz/ACqei88ipFv9VZbaGW4htLZrxkuryO9luYMeSCm2QsjKC3UbuGwMnNLp/XewPQ6251tbXUZ7I2N3LJFaNdIYlVvOVSAVQbs7skcEAc9amv8ATrPXNN+zalbeZDJh9j5VkbqCCDlWB6EEEHoa469uYYLlobrVxJMmgz/6Qsxhkb51KsCG3DgDBzz171HeanAPtE39tSq//CPi42i/YDzQDtcDdwfp1754qeZJa/18X/yIK7dv6+z/AJneWlpb2FnFaWUKQW8KBI4o1wqKOgAqR5UjZFdgDI21Qe5wTj8ga4qG8v8AUdacy6xHaxg2z2SoGY3KFVLlQHCtlt6nKtgY6cVf8WwW/wDamhXV3JcRxRXMgdorl4usMmB8rDkkAD1zjvV2Fe6udRUE10Ybq3h+zzSeezL5ka5WPAJyxzwDjA965S71G/Wa8KyMuqJqcUVpaeaQHty0eTszgjaZCWwcEHn5eL/iW5it9d8PLJevbCa7kjkQXLRiRDBIeQCAcMFwT0OMYpLVX/ra41rf5/gdHRXA6BqcLvokP9sSTm7N5FIHvmdnVWITq2cgAYYc+/NGiXElza+H4bTULu6muEkTUVa8kdli8tvmJ3ZRt4QBhgnJ+tK+tgWqud9TYpUmiWWJg6OMqw7ivP8ARbwT3egWd7qVy1xd6FMlxE97IGeVTFjI3cSDL8/e6+la3gq8sNP8HaPA88okkjhgPmyvJ+/MYJQZJ242n5eAKvl/r5tfp+I2rHWUVB9ttxqIsfNH2kxecI8H7mcZz06mi3vbe7kuI7eQO1tJ5UoAPyPgNj8mH51IieiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAEdSyMoYqSMBhjI9+ap6RpiaPpUNhFPNPHCNqvNt3Y9PlAH6VdooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAP/2Q==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(\"Images/ig.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with accumulating gradients in the second part of the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute gradients for these 10 points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_l = []\n",
    "count = 0\n",
    "for row in delta_points_np:\n",
    "    delta_input = torch.tensor([row],requires_grad=True)\n",
    "    delta_output = net(delta_input) #[1, 2] [batch_size,num_classes]\n",
    "    target = delta_output[0][target_index].unsqueeze(-1) #[1] taking the \n",
    " \n",
    "    # Now compute gradients of the input with respect to the output:\n",
    "    d_loss_dx = grad(outputs=target, inputs=delta_input) # it is a tuple of which first element is gradient of delta_input\n",
    "    gradient_l.append(d_loss_dx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating a function that takes in a list of gradient tensors and returns the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTensorMean(TensorList):\n",
    "    tensor_2D = torch.stack(TensorList)\n",
    "    return torch.mean(tensor_2D,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The means of gradients of 10 vectors between input and baseline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0075, -0.0061, -0.0054]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_l_mean = getTensorMean(gradient_l)\n",
    "print(\"The means of gradients of 10 vectors between input and baseline\")\n",
    "gradient_l_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take the difference between input and baseline. This is a straight line between input and baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2000, -0.4000,  0.4000]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = (inData-baseline)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying it by the difference of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions from simplified version of the code\n",
      "\t 0.0015\n",
      "\t 0.00244\n",
      "\t -0.00217\n"
     ]
    }
   ],
   "source": [
    "attributions = gradient_l_mean * diff\n",
    "print(\"Attributions from simplified version of the code\")\n",
    "for att in attributions[0]:\n",
    "    print(\"\\t\", round(att.item(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above attributions are close to the ones from the library in **cell 22**. Next we can expand the library code in the section below to understand what the library does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanded Library Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of taking equally spaced points , we generate these points with gausian legrande function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Datapoints and target vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = list(0.5 * np.polynomial.legendre.leggauss(num_steps)[1])\n",
    "alphas = list(0.5 * (1 + np.polynomial.legendre.leggauss(num_steps)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equally spaced\n",
      "tensor([[0.0000],\n",
      "        [0.1111],\n",
      "        [0.2222],\n",
      "        [0.3333],\n",
      "        [0.4444],\n",
      "        [0.5556],\n",
      "        [0.6667],\n",
      "        [0.7778],\n",
      "        [0.8889],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Equally spaced\")\n",
    "print (dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gausian legrande function\n",
      "[0.013046735741414128, 0.06746831665550773, 0.16029521585048778, 0.2833023029353764, 0.4255628305091844, 0.5744371694908156, 0.7166976970646236, 0.8397047841495122, 0.9325316833444923, 0.9869532642585859]\n"
     ]
    }
   ],
   "source": [
    "print(\"gausian legrande function\")\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 10 points between input and baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_points =torch.cat([baseline + alpha * (inData - baseline) for alpha in alphas],dim=0).requires_grad_() #[10,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a tensor for the required target class as follows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.cat(num_steps* [torch.tensor([target_index])]) #[num_steps] [10]\n",
    "target = target.reshape(num_steps, 1) #[num_steps,1] [10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running through Ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the probability of target class occuring in each of the 10 vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5175],\n",
       "        [0.5176],\n",
       "        [0.5178],\n",
       "        [0.5180],\n",
       "        [0.5183],\n",
       "        [0.5185],\n",
       "        [0.5188],\n",
       "        [0.5190],\n",
       "        [0.5192],\n",
       "        [0.5193]], grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(delta_points)\n",
    "target_Dp = torch.gather(output, 1, target)\n",
    "assert target_Dp[0].numel() == 1\n",
    "target_Dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining gradients of the output with respect to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = torch.autograd.grad(torch.unbind(target_Dp), delta_points) # torch.unbind removes one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient we calculated in simplified version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0076, -0.0062, -0.0055]]),\n",
       " tensor([[ 0.0076, -0.0062, -0.0055]]),\n",
       " tensor([[ 0.0076, -0.0061, -0.0055]]),\n",
       " tensor([[ 0.0075, -0.0061, -0.0055]]),\n",
       " tensor([[ 0.0075, -0.0061, -0.0054]]),\n",
       " tensor([[ 0.0075, -0.0061, -0.0054]]),\n",
       " tensor([[ 0.0075, -0.0061, -0.0054]]),\n",
       " tensor([[ 0.0074, -0.0060, -0.0054]]),\n",
       " tensor([[ 0.0074, -0.0060, -0.0054]]),\n",
       " tensor([[ 0.0074, -0.0060, -0.0054]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The gradient we calculated in simplified version\")\n",
    "gradient_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gradient we calculated using library functions in cell 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0076, -0.0062, -0.0055],\n",
       "         [ 0.0076, -0.0062, -0.0055],\n",
       "         [ 0.0076, -0.0061, -0.0055],\n",
       "         [ 0.0076, -0.0061, -0.0055],\n",
       "         [ 0.0075, -0.0061, -0.0054],\n",
       "         [ 0.0075, -0.0061, -0.0054],\n",
       "         [ 0.0075, -0.0061, -0.0054],\n",
       "         [ 0.0074, -0.0060, -0.0054],\n",
       "         [ 0.0074, -0.0060, -0.0054],\n",
       "         [ 0.0074, -0.0060, -0.0054]]),)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"the gradient we calculated using library functions in cell 11\")\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled_grads seems to be smoothing the values by multiplying with step sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0003, -0.0002, -0.0002],\n",
       "         [ 0.0006, -0.0005, -0.0004],\n",
       "         [ 0.0008, -0.0007, -0.0006],\n",
       "         [ 0.0010, -0.0008, -0.0007],\n",
       "         [ 0.0011, -0.0009, -0.0008],\n",
       "         [ 0.0011, -0.0009, -0.0008],\n",
       "         [ 0.0010, -0.0008, -0.0007],\n",
       "         [ 0.0008, -0.0007, -0.0006],\n",
       "         [ 0.0006, -0.0004, -0.0004],\n",
       "         [ 0.0002, -0.0002, -0.0002]], dtype=torch.float64)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattening grads so that we can multilpy it with step-size\n",
    "# calling contiguous to avoid `memory whole` problems\n",
    "n_steps=10\n",
    "scaled_grads = [\n",
    "    grad.contiguous().view(n_steps, -1)\n",
    "    * torch.tensor(step_sizes).view(n_steps, 1).to(grad.device)\n",
    "    for grad in grads\n",
    "]\n",
    "scaled_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "def _reshape_and_sum(\n",
    "    tensor_input: Tensor, num_steps: int, num_examples: int, layer_size: Tuple[int, ...]\n",
    ") -> Tensor:\n",
    "    # Used for attribution methods which perform integration\n",
    "    # Sums across integration steps by reshaping tensor to\n",
    "    # (num_steps, num_examples, (layer_size)) and summing over\n",
    "    # dimension 0. Returns a tensor of size (num_examples, (layer_size))\n",
    "    print(\"layer_size\",layer_size)\n",
    "    print(\"num_examples\",num_examples)\n",
    "    print(\"num_steps\",num_steps)\n",
    "    return torch.sum(\n",
    "        tensor_input.reshape((num_steps, num_examples) + layer_size), dim=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_size torch.Size([3])\n",
      "num_examples 1\n",
      "num_steps 10\n",
      "Total gradients after smoothing it out a bit\n",
      "\t 0.0075060081458132085\n",
      "\t -0.006091049426064586\n",
      "\t -0.005426962098435991\n"
     ]
    }
   ],
   "source": [
    "# aggregates across all steps for each tensor in the input tuple\n",
    "# total_grads has the same dimensionality as inputs\n",
    "total_grads = [\n",
    "    _reshape_and_sum(\n",
    "        tensor_input=scaled_grad, num_steps=n_steps, num_examples=grad.shape[0] // n_steps, layer_size=grad.shape[1:] # // is integer division\n",
    "    )\n",
    "    for (scaled_grad, grad) in zip(scaled_grads, grads)\n",
    "]\n",
    "print (\"Total gradients after smoothing it out a bit\")\n",
    "for tg in total_grads[0][0]:\n",
    "    print(\"\\t\",tg.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing it with the mean we generated in cell 13\n",
      "\t 0.007505772169679403\n",
      "\t -0.006090858019888401\n",
      "\t -0.005426791496574879\n"
     ]
    }
   ],
   "source": [
    "print (\"Comparing it with the mean we generated in cell 13\")\n",
    "for tg in gradient_l_mean[0]:\n",
    "    print(\"\\t\",tg.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes attribution for each tensor in input tuple\n",
    "# attributions has the same dimensionality as inputs\n",
    "attributions_expanded_code = tuple(\n",
    "    total_grad * (input - base)\n",
    "    for total_grad, input, base in zip(total_grads, inData, baseline)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions from library code\n",
      "\t 0.0015\n",
      "\t 0.00244\n",
      "\t -0.00217\n",
      "Attributions from simplified version of the code\n",
      "\t 0.0015\n",
      "\t 0.00244\n",
      "\t -0.00217\n",
      "attributions from expanding library code\n",
      "\t 0.0015\n",
      "\t 0.00244\n",
      "\t -0.00217\n"
     ]
    }
   ],
   "source": [
    "print(\"Attributions from library code\")\n",
    "for att in attributions_summary[0]:\n",
    "    print(\"\\t\", round(att.item(),5))\n",
    "\n",
    "print(\"Attributions from simplified version of the code\")\n",
    "for att in attributions[0]:\n",
    "    print(\"\\t\", round(att.item(),5))\n",
    "    \n",
    "print(\"attributions from expanding library code\")\n",
    "for att in attributions_expanded_code[0][0]:\n",
    "    print(\"\\t\", round(att.item(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. https://distill.pub/2020/attribution-baselines/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpret_k1",
   "language": "python",
   "name": "interpret_k1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
