{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple classification Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNN,self).__init__()\n",
    "        self.input_linear = nn.Linear(in_features=3,out_features=3,bias=True) # takes 2 features and outputs 3 \n",
    "        self.hidden= nn.Linear(in_features=3,out_features=2,bias=True) #one output for each class\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.input_linear(x))\n",
    "        x= torch.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleNN(\n",
      "  (input_linear): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (hidden): Linear(in_features=3, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = simpleNN()\n",
    "print(net) # this is the archetecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next lets look at the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of parameters  4\n",
      "Weight matrix for input features:  [3, 3]\n",
      "\t tensor([[-0.5458,  0.0255, -0.4028],\n",
      "Bias for input to hidden layer mapping:  [3]\n",
      "\t tensor([0.2715, 0.4314, 0.3151], requires_grad=True)\n",
      "Weight matrix for hidden to output:1*3 [2, 3]\n",
      "\t tensor([[-0.4984, -0.5088, -0.3487],\n",
      "Bias for  hidden to output:1 \n",
      "\t tensor([-0.0551, -0.4452], requires_grad=True) [2]\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(\" Number of parameters \",len(params))\n",
    "print(\"Weight matrix for input features: \",list(params[0].shape))\n",
    "print(\"\\t\",str(params[0]).split('\\n')[1])\n",
    "print(\"Bias for input to hidden layer mapping: \",list(params[1].shape))\n",
    "print(\"\\t\",str(params[1]).split('\\n')[1])\n",
    "print(\"Weight matrix for hidden to output:1*3\",list(params[2].shape))\n",
    "print(\"\\t\",str(params[2]).split('\\n')[1])\n",
    "print(\"Bias for  hidden to output:1 \")\n",
    "print(\"\\t\",str(params[3]).split('\\n')[1],list(params[3].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inData = torch.tensor([[ 0.5, -0.3,0.2]],requires_grad=True)\n",
    "target_label = torch.Tensor([[0,1]]) # we are saying that this record should be classified as the second class\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward + BackPropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleNN.input_linear.bias.grad before backward\n",
      "None\n",
      "simpleNN.input_linear.bias.grad after backward\n",
      "tensor([0.0299, 0.0000, 0.0463])\n"
     ]
    }
   ],
   "source": [
    "net = simpleNN() #instantiate\n",
    "output = net(inData) # call forward function once\n",
    "#A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "criterion = nn.MSELoss() # more on this later\n",
    "loss = criterion(output, target_label)\n",
    "#Zero the gradient buffers of all parameters\n",
    "net.zero_grad()\n",
    "#Backward is the function which actually calculates the gradient by passing it's argument (1x1 unit tensor by default) through the backward graph all the way up to every leaf node traceable from the calling root tensor. \n",
    "# when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient.\n",
    "print('simpleNN.input_linear.bias.grad before backward')\n",
    "print(net.input_linear.bias.grad)\n",
    "loss.backward() # creates the gradients and stores it in parameters\n",
    "print('simpleNN.input_linear.bias.grad after backward')\n",
    "print(net.input_linear.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "# we can access its gradients like we did before.\n",
    "with torch.no_grad():\n",
    "    for param in net.parameters():\n",
    "        param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from library tensor(0.2696, grad_fn=<MseLossBackward>)\n",
      "diff between output and actual target tensor([[ 0.5192, -0.5192]], grad_fn=<SubBackward0>)\n",
      "taking square tensor([[0.2696, 0.2696]], grad_fn=<PowBackward0>)\n",
      "taking default mean for MSE loss tensor(0.2696, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = criterion(output, target_label) \n",
    "print(\"Loss from library\",loss)\n",
    "print(\"diff between output and actual target\",(output - target_label))\n",
    "print(\"taking square\",(output - target_label)**2 )\n",
    "print(\"taking default mean for MSE loss\" ,torch.mean((output - target_label)**2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpret_k1",
   "language": "python",
   "name": "interpret_k1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
